---
title: "521 Proj 2"
author: "Xuzhang 'Gavin' Li"
date: "2022-12-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally)
library(dplyr)
library(patchwork)
library(dplyr)
library(ggplot2)
library(hrbrthemes)
library(tidyr)
library(viridis)
Sys.setenv(LANG = "en")

```

## 1(b)
```{r}
# Load data
#Image1
image1 <- read.table('imagem1.txt', header = F )
names <- c("y_coordinate", "x_coordinate", "expert_label", 
            "NDAI", "SD", "CORR", "DF","CF", "BF","AF", "AN")
colnames(image1) <- names

#Image2
image2 <- read.table('imagem2.txt', header = F )
colnames(image2) <- names

#Image3
image3 <- read.table('imagem3.txt', header = F )
colnames(image3) <- names

sum(image1$expert_label == -1)/dim(image1)[1]
sum(image1$expert_label == 0)/dim(image1)[1]
sum(image1$expert_label == 1)/dim(image1)[1]

sum(image2$expert_label == -1)/dim(image1)[1]
sum(image2$expert_label == 0)/dim(image1)[1]
sum(image2$expert_label == 1)/dim(image1)[1]

sum(image3$expert_label == -1)/dim(image1)[1]
sum(image3$expert_label == 0)/dim(image1)[1]
sum(image3$expert_label == 1)/dim(image1)[1]
ggplot(image1, aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label)))+
  geom_point()+
  labs(color = "group")+
  scale_color_manual(values=c('Gray','Black','White'),
                     labels = c("clear", "unlabelled","cloud"))

ggplot(image2, aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label)))+
  geom_point()+
  labs(color = "group")+
  scale_color_manual(values=c('Gray','Black','White'),
                     labels = c("clear", "unlabelled","cloud"))

ggplot(image3, aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label)))+
  geom_point()+
  labs(color = "group")+
  scale_color_manual(values=c('Gray','Black','White'),
                     labels = c("clear", "unlabelled","cloud"))
```

## 1(c)(i)
```{r}
ggpairs(image1[c("NDAI","SD","CORR","DF","CF","BF","AF","AN")])
ggpairs(image2[c("NDAI","SD","CORR","DF","CF","BF","AF","AN")])
ggpairs(image3[c("NDAI","SD","CORR","DF","CF","BF","AF","AN")])
```

##1(c)(ii)
```{r}
ggpairs(image1[c("expert_label","NDAI")])
ggpairs(image2[c("expert_label","NDAI")])
ggpairs(image3[c("expert_label","NDAI")])
ggpairs(image1[c("expert_label","SD")])
ggpairs(image2[c("expert_label","SD")])
ggpairs(image3[c("expert_label","SD")])
ggpairs(image1[c("expert_label","CORR")])
ggpairs(image2[c("expert_label","CORR")])
ggpairs(image3[c("expert_label","CORR")])

image1_refine <- image1 %>% filter(expert_label != 0)
image2_refine <- image2 %>% filter(expert_label != 0)
image3_refine <- image3 %>% filter(expert_label != 0)
p1 = ggplot(image1_refine, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image1: log SD")

p2 = ggplot(image2_refine, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image2: log SD")

p3 = ggplot(image3_refine, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image3: log SD")

p1 + p2 + p3 + plot_layout(nrow=3)
rm(p1,p2,p3)
```

```{r}
p4 = ggplot(image1_refine, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image1: NDAI")

p5 = ggplot(image2_refine, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image2: NDAI")

p6 = ggplot(image3_refine, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image3: NDAI")

p4 + p5 + p6 + plot_layout(nrow=3)
```

```{r}
p7 = ggplot(image1_refine, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image1:  CORR")

p8 = ggplot(image2_refine, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image2: CORR")

p9 = ggplot(image3_refine, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image3: CORR")

p7 + p8 + p9 + plot_layout(nrow=3)
```

```{r}
p10 = ggplot(image1_refine, aes(x = DF, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image1: DF")

p11 = ggplot(image2_refine, aes(x = DF, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image2: DF")

p12 = ggplot(image3_refine, aes(x = DF, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  ggtitle("Image3: DF")

p10 + p11 + p12 + plot_layout(nrow=3)
```

```{r}
image12 <- rbind(image1, image2)
t <- rbind(image12, image3)
t12 <- rbind(image1_refine, image2_refine)
t <- rbind(t12, image3_refine)
```

## 2. Preparation

### 2.1

To split the entire data, including all three data sets (the three images), imagem1.txt, imagem2.txt, and imagem3.txt, into training, validation, and testing sets, it is important to note that simple i.i.d method of separation would not be optimal due to the strong spatial correlation of the data. In order to preserve the spatial correlation of the data, block cv would be efficient as it maintains the spatial structure of the data and the data points in each block would be roughly more similar to each other, and ensure that in most cases a pixel would be in the same block as its close neighbors, so the spatial integrity of the data is preserved.

Since the three data sets represents three different images, the most natural and standard way of spliting the data with block cv would cut each of the images (data sets) into blocks, and randomly put the blocks into training, validation, and testing sets. Specifically, each of the three images are split into 16 roughly even rectangular blocks, with 12 of the blocks randomly selected to be the training set, 2 of the other 4 blocks randomly selected to be the testing set, and the rest 2 blocks selected to be the testing set. With this random block selection done to each of the three images, there would be a training set containing data from 36 blocks (12 from each of the images), a validation set and a testing set each containing 6 blocks (2 from each of the images), with the separation of blocks completely random.

An alternative method of data splitting would also use the block cv technique as spatial correlation is definitely needed to be preserved in this analysis. Instead of randomly select 12 blocks, 2 blocks, and 2 blocks into the training, validation, and testing set respectively from each of the images, it also makes sense to simply split each image into 64 roughly even blocks, mix up all 192 blocks, and then randomly select 144 blocks as the training set, 24 blocks as the validation set, and 24 blocks as the testing set. This way, the proportion of data in each set would be more random and as it is likely not to be evenly distributed among the images. As the number of blocks were also increased, the randomization was further strengthened and closer to simple i.i.d, while the spatial relation of the data is captured to a lesser extent. Also as the three images might be inherently different in their properties, this methods contains the risk of separating too much of a specific image into a certain set and produce a rather skewed result. 

For the following analysis, the first method of data separation is used. Also note that data points without an expert label (i.e. with an unlabeled expert model) are removed. They provide no valuable information to the model and cross-validation as classification success and failure rate all depends o the existence of an expert label. Note that this removed 137495 rows out of 345556 rows, so a significant portion of the data was removed due to unlabeled expert labels.

```{r r1}
set.seed(123)
t1b <- image1_refine
t1b$block <- 4*(t1b$y_coordinate %/% 96) + (t1b$x_coordinate %/% 93) + 1
trainvalidblocks <- sample(1:16, replace = F, size = 14)
trainblocks <- sample(trainvalidblocks, replace = F, size = 12)
validblocks <- setdiff(trainvalidblocks, trainblocks)
testblocks <- setdiff(1:16, trainvalidblocks)
t1_train <- t1b[t1b$block %in% trainblocks,]
t1_valid <- t1b[t1b$block %in% validblocks,]
t1_test <- t1b[t1b$block %in% testblocks,]

t2b <- image2_refine
t2b$block <- 4*(t2b$y_coordinate %/% 96) + (t2b$x_coordinate %/% 93) + 17
trainvalidblocks <- sample(17:32, replace = F, size = 14)
trainblocks <- sample(trainvalidblocks, replace = F, size = 12)
validblocks <- setdiff(trainvalidblocks, trainblocks)
testblocks <- setdiff(17:32, trainvalidblocks)
t2_train <- t2b[t2b$block %in% trainblocks,]
t2_valid <- t2b[t2b$block %in% validblocks,]
t2_test <- t2b[t2b$block %in% testblocks,]

t3b <- image3_refine
t3b$block <- 4*(t3b$y_coordinate %/% 96) + (t3b$x_coordinate %/% 93) + 33
trainvalidblocks <- sample(33:48, replace = F, size = 14)
trainblocks <- sample(trainvalidblocks, replace = F, size = 12)
validblocks <- setdiff(trainvalidblocks, trainblocks)
testblocks <- setdiff(33:48, trainvalidblocks)
t3_train <- t3b[t3b$block %in% trainblocks,]
t3_valid <- t3b[t3b$block %in% validblocks,]
t3_test <- t3b[t3b$block %in% testblocks,]


train_set1 <- rbind(rbind(t1_train, t2_train), t3_train)
valid_set1 <- rbind(rbind(t1_valid, t2_valid), t3_valid)
train_valid_set1 <- rbind(train_set1,valid_set1)
train_valid_set1$expert_label <- (train_valid_set1$expert_label+1)/2
test_set1 <- rbind(rbind(t1_test, t2_test), t3_test)
test_set1$expert_label <- (test_set1$expert_label+1)/2


set.seed(111)
t1b <- image1_refine

t1b$block <- 8*(t1b$y_coordinate %/% 48) + (t1b$x_coordinate %/% 47) + 1
t2b <- image2_refine
t2b$block <- 8*(t2b$y_coordinate %/% 48) + (t2b$x_coordinate %/% 47) + 65
t3b <- image3_refine
t3b$block <- 8*(t3b$y_coordinate %/% 48) + (t3b$x_coordinate %/% 47) + 129
tb <- rbind(rbind(t1b, t2b), t3b)

trainvalidblocks <- sample(1:192, replace = F, size = 168)
trainblocks <- sample(trainvalidblocks, replace = F, size = 144)
validblocks <- setdiff(trainvalidblocks, trainblocks)
testblocks <- setdiff(1:192, trainvalidblocks)
train_set2 <- tb[tb$block %in% trainblocks,]
valid_set2 <- tb[tb$block %in% validblocks,]
train_valid_set2 <- rbind(train_set2, valid_set2)
train_valid_set2$expert_label <- (train_valid_set2$expert_label+1)/2
test_set2 <- tb[tb$block %in% testblocks,]
test_set2$expert_label <- (test_set2$expert_label+1)/2
```

### 2.2

The trivial classifier was introduced which simple sets all classified labels to be -1, which represent cloud-free, on the validation and testing set. The accuracy of this trivial classifier is also trivial in some sense as it simply captures the proportion of -1 expert label (the proportion of cloud-free expert labels) in the validation set and the testing set. Again, as noted previously, all 137495 data points with unlabeled expert labels were removed as they contribute no valuable information to model construction and cross-validation.

In the case of this trivial classifier, the validation set classification accuracy is 0.632, and the test set classification accuracy is 0.784. When a certain label makes up a large proportion of data, these kind of trivial classifiers would actually have high average accuracy. In this case, this trivial classifier provided rather moderately high classification accuracy, and serves as a baseline of classification accuracy that the cross-validation trained models need to improve upon. This kind of trivial classifier baseline is important as sometimes even a high 95% model classification rate would be weak if a trivial classifier can produce similar accuracy.

```{r}
dim(filter(valid_set1, expert_label == -1))[1]/dim(valid_set1)[1]
dim(filter(test_set1, expert_label == 0))[1]/dim(test_set1)[1]
```

### 2.3

```{r}
#t_rm <- t[!is.na(t$expert_label),]
tdf <- as.data.frame(t)
tdf$expert_label <- (tdf$expert_label + 1)/2
reg_ols <- lm(expert_label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, data=tdf)
log_reg <- glm(expert_label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, data = tdf, family = "binomial")

log_reg
reg_ols

t1df <- as.data.frame(image1_refine)
t1df$expert_label <- (t1df$expert_label + 1)/2
reg_ols <- lm(expert_label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, data=t1df)
log_reg <- glm(expert_label ~ NDAI+SD+CORR+DF+CF+BF+AF+AN, data = t1df, family = "binomial")


## PCA
pca_direc <- prcomp(tdf[, 4:11], center = TRUE, scale = TRUE)
loadings <- pca_direc$rotation
loadings[, 1:3]
scores <- pca_direc$x
scores[, 1:3]
eigenvalues <- pca_direc$sdev^2
eigenvalues
ggplot() + geom_line(aes(x=seq(1:length(eigenvalues)),
                         y=cumsum(eigenvalues)/sum(eigenvalues))) +
  labs(x='Number of PCs', y='Total percentange variability')

```

```{r}
ggplot(t) +
geom_point(aes(x=NDAI, y=CORR, color=as.factor(expert_label)), alpha=0.8)
#ggplot(t) +
#geom_point(aes(x=NDAI, y=SD, color=as.factor(expert_label)), alpha=0.8)
#ggplot(t) +
#eom_point(aes(x=SD, y=CORR, color=as.factor(expert_label)), alpha=0.8)

#ggplot(t) +
#geom_point(aes(x=NDAI, y=y_coordinate1, color=as.factor(expert_label)), alpha=0.8)
#ggplot(t) +
#geom_point(aes(x=SD, y=y_coordinate1, color=as.factor(expert_label)), alpha=0.8)
#ggplot(t) +
#geom_point(aes(x=CORR, y=y_coordinate1, color=as.factor(expert_label)), alpha=0.8)

```

Based on both the regular ols and the logistic regression result, it is clear that NDAI, CORR, and y_coordinate1 are the two most significant predictor variables for the expert label expert_label, while the other variables seem much less significant. Based off the PCA, the first principal component contains roughly 75% of the variability of the data set, with (-0.31, -0.28, - 0.27, 0.26, 0.38, 0.42, 0.43, 0.42). With each variable having a coefficient within 0.28-0.43, the weight is roughly even and there isn't much to conclude from the PCA that would add to the logistic regression.

## 2.4
```{r}
library(caret)
library(MASS)
library(e1071)
library(tree)
library(randomForest)
CVmaster <- function(clas_func, features, labels, K, loss_func, split_opt, tr1, te1, tr2, te2){
      train_valid_set1 <- tr1
      train_valid_set2 <- tr2
      test_set1 <- te1
      test_set2 <- te2
    if (split_opt == 1){
      blocks <- unique(train_valid_set1$block)
      folds <- createFolds(blocks, k = K)
      currFormula <- as.formula(paste(labels, "~",
                                      paste(features[], collapse = "+"), sep = ""))
      loss = rep(0, K)
      for (i in 1:K){
        Val <- train_valid_set1[train_valid_set1$block %in% folds[[i]],]
        Train <- train_valid_set1[!train_valid_set1$block %in% folds[[i]],]
        true <- Val$expert_label
        if(clas_func == "glm"){
          model <- glm(currFormula, data = Train, family = "binomial")
          logistic_prob <- predict(model, Val, type="response")
          logistic_pred <- 0*1:nrow(Val)
          logistic_pred[logistic_prob>0.5] = 1
          loss[i] = mean(logistic_pred != true)
        }
        if(clas_func == "lda"){
          model <- lda(currFormula, data = Train)
          pred <- predict(model, Val)
          loss[i] = mean(pred$class != true)
        }
        if(clas_func == "qda"){
          model <- qda(currFormula, data = Train)
          pred <- predict(model, Val)
          loss[i] = mean(pred$class != true)
        }
        if(clas_func == "nb"){
          model <- naiveBayes(currFormula, data = Train)
          pred <- predict(model, Val)
          loss[i] = mean(pred != true)
        }
        if(clas_func == "tree"){
          model <- tree(currFormula, data=Train)
          pred <- predict(model, Val, 'vector')
          pred[pred < 0.5] = 0
          pred[pred > 0.5] = 1
          loss[i] = mean(pred != true)
        }
      }
      return (loss)
    }
    if (split_opt == 2){
      blocks <- unique(train_valid_set2$block)
      folds <- createFolds(blocks, k = K)
      currFormula <- as.formula(paste(labels, "~",
                                      paste(features[], collapse = "+"),
                                      sep = ""))
      loss = rep(0, K)
      for (i in 1:K){
        Val <- train_valid_set2[train_valid_set2$block %in% folds[[i]],]
        Train <- train_valid_set2[!train_valid_set2$block %in% folds[[i]],]
        true <- Val$expert_label
        if(clas_func == "glm"){
          model <- glm(currFormula, data = Train, family = "binomial")
          logistic_prob <- predict(model, Val, type="response")
          logistic_pred <- 0*1:nrow(Val)
          logistic_pred[logistic_prob>0.5] = 1
          loss[i] = mean(logistic_pred != true)
        }
        if(clas_func == "lda"){
          model <- lda(currFormula, data = Train)
          pred <- predict(model, Val)
          loss[i] = mean(pred$class != true)
        }
        if(clas_func == "qda"){
          model <- qda(currFormula, data = Train)
          pred <- predict(model, Val)
          loss[i] = mean(pred$class != true)
        }
        if(clas_func == "nb"){
          model <- naiveBayes(currFormula, data = Train)
          pred <- predict(model, Val)
          loss[i] = mean(pred != true)
        }
        if(clas_func == "tree"){
          model <- tree(currFormula, data=Train)
          pred <- predict(model, Val, 'vector')
          pred[pred < 0.5] = 0
          pred[pred > 0.5] = 1
          loss[i] = mean(pred != true)
        }
      }
      return (loss)
    }
}
MSE <- function(a, b){
  return (mean((a-b)^2))
}

```

## 3.1
```{r}
     tr1<- train_valid_set1
     tr2<-train_valid_set2
     te1<-test_set1 
     te2<-test_set2
glm_acc <- 1- CVmaster(clas_func = "glm", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 1, tr1, te1, tr2, te2)
lda_acc <- 1- CVmaster(clas_func = "lda", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 1, tr1, te1, tr2, te2)
qda_acc <- 1- CVmaster(clas_func = "qda", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 1, tr1, te1, tr2, te2)
nb_acc <- 1- CVmaster(clas_func = "nb", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 1, tr1, te1, tr2, te2)
tree_acc <- 1- CVmaster(clas_func = "tree", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 1, tr1, te1, tr2, te2)


glm_acc2 <- 1- CVmaster(clas_func = "glm", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 2, tr1, te1, tr2, te2)
lda_acc2 <- 1- CVmaster(clas_func = "lda", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 2, tr1, te1, tr2, te2)
qda_acc2 <- 1- CVmaster(clas_func = "qda", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 2, tr1, te1, tr2, te2)
nb_acc2 <- 1- CVmaster(clas_func = "nb", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 2, tr1, te1, tr2, te2)
tree_acc2 <- 1- CVmaster(clas_func = "tree", features = c("NDAI","SD","CORR"), labels = "expert_label", K = 5, 
         loss_func = MSE, split_opt = 2, tr1, te1, tr2, te2)

fold_num <- c("fold1","fold2", "fold3", "fold4", "fold5")
split1_acc <- data.frame(fold_num, glm_acc, lda_acc, qda_acc, nb_acc,tree_acc)
split2_acc <- data.frame(fold_num, glm_acc2, lda_acc2, qda_acc2, nb_acc2,tree_acc2)
split1_acc
split2_acc

mean(glm_acc)
mean(lda_acc)
mean(qda_acc)
mean(nb_acc)
mean(tree_acc)

mean(glm_acc2)
mean(lda_acc2)
mean(qda_acc2)
mean(nb_acc2)
mean(tree_acc2)
```
The split1_acc and split2_acc are the fold 1-5 accuracies for spliiting methods 1 and 2 respectively.

```{r}
library(pROC)
test_acc1 <- rep(0, 5)
true <- test_set1$expert_label
glm1 <- glm(expert_label~NDAI+SD+CORR,data=train_valid_set1, family = 'binomial') 
pred <- predict(glm1, test_set1, type = "response")
test_acc1[1] <- mean((pred > 0.5) == true)
glm1_roc_score=roc(true, pred) #AUC score
plot(glm1_roc_score ,main ="ROC curve -- Logistic Regression", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

lda1 <- lda(expert_label~NDAI+SD+CORR,train_valid_set1) 
pred <- predict(lda1, test_set1)
test_acc1[2] <- mean(pred$class == true)
lda1_roc_score=roc(true, as.numeric(pred$x)) #AUC score
plot(lda1_roc_score ,main ="ROC curve -- LDA", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

qda1 <- qda(expert_label~NDAI+SD+CORR,train_valid_set1) 
pred <- predict(qda1, test_set1)
test_acc1[3] <- mean(pred$class == true)
qda1_roc_score=roc(true, as.numeric(pred$class)) #AUC score
plot(qda1_roc_score ,main ="ROC curve -- QDA", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

nb1 <- naiveBayes(expert_label~NDAI+SD+CORR,train_valid_set1) 
pred <- predict(nb1, test_set1)
test_acc1[4] <- mean(pred == true)
nb1_roc_score=roc(true, as.numeric(pred)) #AUC score
plot(nb1_roc_score ,main ="ROC curve -- Naive Bayes", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

tree1 <- tree(as.factor(expert_label)~NDAI+SD+CORR,train_valid_set1)
pred <- predict(tree1, test_set1, 'class')
test_acc1[5] <- mean(pred == true)
tree1_roc_score=roc(true, as.numeric(pred)) #AUC score
plot(tree1_roc_score ,main ="ROC curve -- Tree", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

ggplot()  + geom_line(aes(x=qda1_roc_score$specificities, y=qda1_roc_score$sensitivities), color="blue") + geom_line(aes(x=glm1_roc_score$specificities,y=glm1_roc_score$sensitivities), color="green") + geom_line(aes(x=nb1_roc_score$specificities,y=nb1_roc_score$sensitivities), color="orange") + geom_line(aes(x=lda1_roc_score$specificities,y=lda1_roc_score$sensitivities), color="red")+
geom_line(aes(x=tree1_roc_score$specificities,y=tree1_roc_score$sensitivities), color="yellow")+ labs(x="False Positive Rate", y = "True Positive Rate", title = "ROC Curve For Method 1, Blue:qda, Red:lda, Orange: naive bayes, Green: logistic","Yellow: tree") 


test_acc2 <- rep(0, 5)
true <- test_set2$expert_label
glm2 <- glm(expert_label~NDAI+SD+CORR,data=train_valid_set2, family = 'binomial') 
pred <- predict(glm2, test_set2, type = "response")
test_acc2[1] <- mean((pred > 0.5) == true)
glm2_roc_score=roc(true, pred) #AUC score
plot(glm2_roc_score ,main ="ROC curve -- Logistic Regression", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

lda2 <- lda(expert_label~NDAI+SD+CORR,train_valid_set2) 
pred <- predict(lda2, test_set2)
test_acc2[2] <- mean(pred$class == true)
lda2_roc_score=roc(true, as.numeric(pred$x)) #AUC score
plot(lda2_roc_score ,main ="ROC curve -- LDA", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

qda2 <- qda(expert_label~NDAI+SD+CORR,train_valid_set2) 
pred <- predict(qda2, test_set2)
test_acc2[3] <- mean(pred$class == true)
qda2_roc_score=roc(true, as.numeric(pred$class)) #AUC score
plot(qda2_roc_score ,main ="ROC curve -- QDA", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

nb2 <- naiveBayes(expert_label~NDAI+SD+CORR,train_valid_set2) 
pred <- predict(nb2, test_set2)
test_acc2[4] <- mean(pred == true)
nb2_roc_score=roc(true, as.numeric(pred)) #AUC score
plot(nb2_roc_score ,main ="ROC curve -- Naive Bayes", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

tree2 <- tree(as.factor(expert_label)~NDAI+SD+CORR,train_valid_set1)
pred <- predict(tree2, test_set2, 'class')
test_acc2[5] <- mean(pred == true)
tree2_roc_score=roc(true, as.numeric(pred)) #AUC score
plot(tree2_roc_score ,main ="ROC curve -- Tree", xlab = "False_Positive_Rate",
     ylab = "True_Positive_Rate")

ggplot()  + geom_line(aes(x=qda2_roc_score$specificities, y=qda2_roc_score$sensitivities), color="blue") + geom_line(aes(x=glm2_roc_score$specificities,y=glm2_roc_score$sensitivities), color="green") + geom_line(aes(x=nb2_roc_score$specificities,y=nb2_roc_score$sensitivities), color="orange") + geom_line(aes(x=lda2_roc_score$specificities,y=lda2_roc_score$sensitivities), color="red") +
geom_line(aes(x=tree2_roc_score$specificities,y=tree1_roc_score$sensitivities), color="yellow")+ labs(x="False Positive Rate", y = "True Positive Rate", title = "ROC Curve For Method 2, Blue:qda, Red:lda, Orange: naive bayes, Green: logistic","Yellow: tree") 
```

```{r}
test_acc1
test_acc2
model_type <- c("logistic","lda", "qda","naive bayes", "tree")
test_acc<-data.frame(model_type,test_acc1, test_acc2)
test_acc
```
And this is the test accuracy of the models.

## 4.1
```{r}
summary(tree1)
plot(tree1)
text(tree1, pretty=0)

true <- test_set1$expert_label
perturb1 <- test_set1
perturb1$NDAI <- sample(perturb1$NDAI)
perturb2 <- test_set1
perturb2$SD <- sample(perturb2$SD)
perturb3 <- test_set1
perturb3$CORR <- sample(perturb3$CORR)

pred1 <- predict(tree1, perturb1, 'class')
pred2 <- predict(tree1, perturb2, 'class')
pred3 <- predict(tree1, perturb3, 'class')


mean(pred1 == true)
mean(pred2 == true)
mean(pred3 == true)

pred4 <- predict(qda1, perturb1)
pred5 <- predict(qda1, perturb2)
pred6 <- predict(qda1, perturb3)

mean(pred4$class == true)
mean(pred5$class == true)
mean(pred6$class == true)

m <- qda1$means
ggplot(t, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  geom_vline(data = as.data.frame(m), aes(xintercept = log(SD)))+
  ggtitle("log SD")

ggplot(t, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  geom_vline(data = as.data.frame(m), aes(xintercept = NDAI))+
  ggtitle("NDAI")

ggplot(t, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()+
  geom_vline(data = as.data.frame(m), aes(xintercept = CORR))+
  ggtitle("CORR")


summary(qda1)
qda1
test_set1 %>% group_by(expert_label)%>%
  summarise(
    NDAI = mean(NDAI),
    SD = mean(SD),
    CORR = mean(CORR)
  )

```

## 4.2
```{r}
true <- test_set1$expert_label
pred <- predict(tree1, test_set1, 'class')
index <- (pred != true)
misclassfi <- test_set1[index,]
mean(misclassfi$expert_label == 1)
p1 = ggplot(test_set1, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()

p2 = ggplot()+
  geom_density(data = misclassfi, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))

p1 + p2 + plot_layout(nrow=2)


p3 = ggplot(test_set1, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()

p4 = ggplot()+
  geom_density(data = misclassfi, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))

p3 + p4 + plot_layout(nrow=2)

p5 = ggplot(test_set1, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()

p6 = ggplot()+
  geom_density(data = misclassfi, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))

p5 + p6 + plot_layout(nrow=2)

ggplot(test_set1, aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label)))+
  geom_point()+
  labs(color = "group")+
  scale_color_manual(values=c('Gray','White', "Blue"),
                     labels = c("clear", "cloud","Error"))+
  geom_point(data = misclassfi, aes(x = x_coordinate, y = y_coordinate, color = "Blue"))
  

```

## 4(d)
```{r}
true <- test_set2$expert_label
pred <- predict(tree2, test_set2, 'class')
index <- (pred != true)
misclassfi <- test_set2[index,]
p1 = ggplot(test_set2, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()

p2 = ggplot()+
  geom_density(data = misclassfi, aes(x = NDAI, fill = as.factor(expert_label), color = as.factor(expert_label)))

p1 + p2 + plot_layout(nrow=2)


p3 = ggplot(test_set2, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()

p4 = ggplot()+
  geom_density(data = misclassfi, aes(x = log(SD), fill = as.factor(expert_label), color = as.factor(expert_label)))

p3 + p4 + plot_layout(nrow=2)

p5 = ggplot(test_set2, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))+
  geom_density()

p6 = ggplot()+
  geom_density(data = misclassfi, aes(x = CORR, fill = as.factor(expert_label), color = as.factor(expert_label)))

p5 + p6 + plot_layout(nrow=2)

ggplot(test_set2, aes(x = x_coordinate, y = y_coordinate, color = as.factor(expert_label)))+
  geom_point()+
  labs(color = "group")+
  scale_color_manual(values=c('Gray','White', "Blue"),
                     labels = c("clear", "cloud","Error"))+
  geom_point(data = misclassfi, aes(x = x_coordinate, y = y_coordinate, color = "Blue"))
  

```